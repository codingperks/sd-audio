{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17 13:01:02,077] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a788ba10105d48598f549caabeaea053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /home/ryan/.cache/huggingface/datasets/imagefolder/default-da7eee62ee4928aa/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e32c2452134146b6e56369e71d09c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b643eecb37ca4bde9811cd01dabc83b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3c5d7a59c7443d97127d889aeb4de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a61a44612a4b91a3e8984909b0b2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /home/ryan/.cache/huggingface/datasets/imagefolder/default-da7eee62ee4928aa/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/home/ryan/diss/msc_diss/sdspeech/data/AudioSet/spec/Bird vocalization-bird call-bird song/train\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17 19:22:05,690] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-07-17 19:22:07,799] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:589: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n",
      "  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n",
      "[2023-07-17 19:22:09,055] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-17 19:22:09,055] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2023-07-17 19:22:09,055] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "07/17/2023 19:22:09 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\n",
      "\n",
      "{'sample_max_value', 'prediction_type', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_only_cross_attention', 'upcast_attention', 'time_embedding_type', 'resnet_out_scale_factor', 'only_cross_attention', 'time_cond_proj_dim', 'time_embedding_dim', 'conv_out_kernel', 'use_linear_projection', 'conv_in_kernel', 'class_embeddings_concat', 'cross_attention_norm', 'encoder_hid_dim', 'num_class_embeds', 'resnet_time_scale_shift', 'projection_class_embeddings_input_dim', 'addition_embed_type', 'encoder_hid_dim_type', 'mid_block_type', 'dual_cross_attention', 'class_embed_type', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'timestep_post_act', 'time_embedding_act_fn'} was not found in config. Values will be initialized to default values.\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [00:00<00:00, 655569.05it/s]\n",
      "07/17/2023 19:22:12 - WARNING - datasets.builder - Found cached dataset imagefolder (/home/ryan/.cache/huggingface/datasets/imagefolder/default-8cf943ace364c9c3/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1409.38it/s]\n",
      "07/17/2023 19:22:13 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (1).\n",
      "Using /home/ryan/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ryan/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.199505567550659 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2023-07-17 19:22:18,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.4, git-hash=unknown, git-branch=unknown\n",
      "07/17/2023 19:22:18 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "07/17/2023 19:22:18 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "[2023-07-17 19:22:18,207] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-07-17 19:22:18,208] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-07-17 19:22:18,208] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-07-17 19:22:18,213] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2023-07-17 19:22:18,213] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2023-07-17 19:22:18,213] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2023-07-17 19:22:18,213] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000\n",
      "[2023-07-17 19:22:18,214] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000\n",
      "[2023-07-17 19:22:18,214] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True\n",
      "[2023-07-17 19:22:18,214] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False\n",
      "Using /home/ryan/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ryan/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07414054870605469 seconds\n",
      "Rank: 0 partition count [1] and sizes[(797184, False)] \n",
      "[2023-07-17 19:22:18,415] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states\n",
      "[2023-07-17 19:22:18,416] [INFO] [utils.py:786:see_memory_usage] MA 2.02 GB         Max_MA 2.02 GB         CA 2.03 GB         Max_CA 2 GB \n",
      "[2023-07-17 19:22:18,416] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 5.89 GB, percent = 37.9%\n",
      "[2023-07-17 19:22:18,518] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states\n",
      "[2023-07-17 19:22:18,519] [INFO] [utils.py:786:see_memory_usage] MA 2.02 GB         Max_MA 2.02 GB         CA 2.03 GB         Max_CA 2 GB \n",
      "[2023-07-17 19:22:18,519] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 5.89 GB, percent = 37.9%\n",
      "[2023-07-17 19:22:18,519] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized\n",
      "[2023-07-17 19:22:18,612] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-07-17 19:22:18,612] [INFO] [utils.py:786:see_memory_usage] MA 2.02 GB         Max_MA 2.02 GB         CA 2.03 GB         Max_CA 2 GB \n",
      "[2023-07-17 19:22:18,613] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 5.89 GB, percent = 37.9%\n",
      "[2023-07-17 19:22:18,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
      "[2023-07-17 19:22:18,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-07-17 19:22:18,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-07-17 19:22:18,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]\n",
      "[2023-07-17 19:22:18,617] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   amp_enabled .................. False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   amp_params ................... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   bfloat16_enabled ............. False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbe39f2c640>\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   communication_data_type ...... None\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   disable_allgather ............ False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   dump_state ................... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   elasticity_enabled ........... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   fp16_auto_cast ............... True\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   fp16_enabled ................. True\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   global_rank .................. 0\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   loss_scale ................... 0\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   memory_breakdown ............. False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   mics_shard_size .............. -1\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-07-17 19:22:18,618] [INFO] [config.py:964:print]   optimizer_name ............... None\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   optimizer_params ............. None\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   pld_enabled .................. False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   pld_params ................... False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   prescale_gradients ........... False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   scheduler_name ............... None\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   scheduler_params ............. None\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   sparse_attention ............. None\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   steps_per_print .............. inf\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   train_batch_size ............. 1\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   use_node_local_storage ....... False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   world_size ................... 1\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   zero_enabled ................. True\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2\n",
      "[2023-07-17 19:22:18,619] [INFO] [config.py:950:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": false\n",
      "    }, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /home/ryan/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0001926422119140625 seconds\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrap\u001b[0m (\u001b[33msteamclock\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ryan/diss/msc_diss/sdspeech/model/sd_ex/lora/wandb/run-20230717_192219-j2zo03sm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msandy-pine-66\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/steamclock/sd_speech\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/steamclock/sd_speech/runs/j2zo03sm\u001b[0m\n",
      "07/17/2023 19:22:24 - INFO - __main__ - ***** Running training *****\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Num examples = 97\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Num Epochs = 600\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "07/17/2023 19:22:24 - INFO - __main__ -   Total optimization steps = 15000\n",
      "Steps:   0%|                                          | 0/15000 [00:00<?, ?it/s]07/17/2023 19:22:24 - INFO - __main__ - Starting epoch 0\n",
      "07/17/2023 19:22:25 - INFO - __main__ - Starting training step 0, global step 0\n",
      "07/17/2023 19:22:26 - INFO - __main__ - train loss is 0.1850908249616623\n",
      "Steps:   0%|               | 0/15000 [00:01<?, ?it/s, lr=0.0001, step_loss=0.74][2023-07-17 19:22:26,367] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "Steps:   0%|     | 1/15000 [00:01<6:26:23,  1.55s/it, lr=0.0001, step_loss=0.74]07/17/2023 19:22:26 - INFO - __main__ - Starting training step 1, global step 1\n",
      "07/17/2023 19:22:26 - INFO - __main__ - train loss is 0.012357455678284168\n",
      "Steps:   0%|   | 1/15000 [00:01<6:26:23,  1.55s/it, lr=0.0001, step_loss=0.0494][2023-07-17 19:22:26,585] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "Steps:   0%|   | 2/15000 [00:01<3:11:06,  1.31it/s, lr=0.0001, step_loss=0.0494]07/17/2023 19:22:26 - INFO - __main__ - Starting training step 2, global step 2\n",
      "07/17/2023 19:22:26 - INFO - __main__ - train loss is 0.1337709277868271\n",
      "Steps:   0%|    | 2/15000 [00:01<3:11:06,  1.31it/s, lr=0.0001, step_loss=0.535][2023-07-17 19:22:26,798] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "Steps:   0%|    | 3/15000 [00:01<2:08:09,  1.95it/s, lr=0.0001, step_loss=0.535]07/17/2023 19:22:26 - INFO - __main__ - Starting training step 3, global step 3\n",
      "07/17/2023 19:22:26 - INFO - __main__ - train loss is 0.0044364845380187035\n",
      "Steps:   0%|   | 3/15000 [00:02<2:08:09,  1.95it/s, lr=0.0001, step_loss=0.0177][2023-07-17 19:22:27,012] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "Steps:   0%|   | 4/15000 [00:02<1:38:39,  2.53it/s, lr=0.0001, step_loss=0.0177]07/17/2023 19:22:27 - INFO - __main__ - Starting training step 4, global step 4\n",
      "07/17/2023 19:22:27 - INFO - __main__ - train loss is 0.0032397201284766197\n",
      "Steps:   0%|    | 4/15000 [00:02<1:38:39,  2.53it/s, lr=0.0001, step_loss=0.013][2023-07-17 19:22:27,226] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "Steps:   0%|    | 5/15000 [00:02<1:22:19,  3.04it/s, lr=0.0001, step_loss=0.013]07/17/2023 19:22:27 - INFO - __main__ - Starting training step 5, global step 5\n",
      "07/17/2023 19:22:27 - INFO - __main__ - train loss is 0.08158817887306213\n",
      "Steps:   0%|    | 5/15000 [00:02<1:22:19,  3.04it/s, lr=0.0001, step_loss=0.326][2023-07-17 19:22:27,441] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "Steps:   0%|    | 6/15000 [00:02<1:12:39,  3.44it/s, lr=0.0001, step_loss=0.326]07/17/2023 19:22:27 - INFO - __main__ - Starting training step 6, global step 6\n",
      "07/17/2023 19:22:27 - INFO - __main__ - train loss is 0.01134561188519001\n",
      "Steps:   0%|   | 6/15000 [00:02<1:12:39,  3.44it/s, lr=0.0001, step_loss=0.0454][2023-07-17 19:22:27,657] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "Steps:   0%|   | 7/15000 [00:02<1:06:29,  3.76it/s, lr=0.0001, step_loss=0.0454]07/17/2023 19:22:27 - INFO - __main__ - Starting training step 7, global step 7\n",
      "07/17/2023 19:22:27 - INFO - __main__ - train loss is 0.06333677470684052\n",
      "Steps:   0%|    | 7/15000 [00:02<1:06:29,  3.76it/s, lr=0.0001, step_loss=0.253][2023-07-17 19:22:27,874] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "Steps:   0%|    | 8/15000 [00:03<1:02:38,  3.99it/s, lr=0.0001, step_loss=0.253]07/17/2023 19:22:27 - INFO - __main__ - Starting training step 8, global step 8\n",
      "07/17/2023 19:22:28 - INFO - __main__ - train loss is 0.0011639914009720087\n",
      "Steps:   0%|  | 9/15000 [00:03<1:00:23,  4.14it/s, lr=0.0001, step_loss=0.00466]07/17/2023 19:22:28 - INFO - __main__ - Starting training step 9, global step 9\n",
      "07/17/2023 19:22:28 - INFO - __main__ - train loss is 0.07386203110218048\n",
      "Steps:   0%|    | 9/15000 [00:03<1:00:23,  4.14it/s, lr=0.0001, step_loss=0.295][2023-07-17 19:22:28,308] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "Steps:   0%|     | 10/15000 [00:03<58:06,  4.30it/s, lr=0.0001, step_loss=0.295]07/17/2023 19:22:28 - INFO - __main__ - Starting training step 10, global step 10\n",
      "07/17/2023 19:22:28 - INFO - __main__ - train loss is 0.10011491179466248\n",
      "Steps:   0%|       | 11/15000 [00:03<57:02,  4.38it/s, lr=0.0001, step_loss=0.4]07/17/2023 19:22:28 - INFO - __main__ - Starting training step 11, global step 11\n",
      "07/17/2023 19:22:28 - INFO - __main__ - train loss is 0.07857954502105713\n",
      "Steps:   0%|     | 12/15000 [00:03<56:35,  4.41it/s, lr=0.0001, step_loss=0.314]07/17/2023 19:22:28 - INFO - __main__ - Starting training step 12, global step 12\n",
      "07/17/2023 19:22:28 - INFO - __main__ - train loss is 0.10938125848770142\n",
      "Steps:   0%|     | 13/15000 [00:04<56:25,  4.43it/s, lr=0.0001, step_loss=0.438]07/17/2023 19:22:28 - INFO - __main__ - Starting training step 13, global step 13\n",
      "07/17/2023 19:22:29 - INFO - __main__ - train loss is 0.004973599687218666\n",
      "Steps:   0%|    | 14/15000 [00:04<55:59,  4.46it/s, lr=0.0001, step_loss=0.0199]07/17/2023 19:22:29 - INFO - __main__ - Starting training step 14, global step 14\n",
      "07/17/2023 19:22:29 - INFO - __main__ - train loss is 0.023529615253210068\n",
      "Steps:   0%|    | 15/15000 [00:04<55:44,  4.48it/s, lr=0.0001, step_loss=0.0941]07/17/2023 19:22:29 - INFO - __main__ - Starting training step 15, global step 15\n",
      "07/17/2023 19:22:29 - INFO - __main__ - train loss is 0.04044240340590477\n",
      "Steps:   0%|     | 16/15000 [00:04<55:29,  4.50it/s, lr=0.0001, step_loss=0.162]07/17/2023 19:22:29 - INFO - __main__ - Starting training step 16, global step 16\n",
      "07/17/2023 19:22:29 - INFO - __main__ - train loss is 0.05655001103878021\n",
      "Steps:   0%|     | 17/15000 [00:05<55:13,  4.52it/s, lr=0.0001, step_loss=0.226]07/17/2023 19:22:29 - INFO - __main__ - Starting training step 17, global step 17\n",
      "07/17/2023 19:22:29 - INFO - __main__ - train loss is 0.026709862053394318\n",
      "Steps:   0%|     | 18/15000 [00:05<55:08,  4.53it/s, lr=0.0001, step_loss=0.107]07/17/2023 19:22:30 - INFO - __main__ - Starting training step 18, global step 18\n",
      "07/17/2023 19:22:30 - INFO - __main__ - train loss is 0.0005299410549923778\n",
      "Steps:   0%|   | 19/15000 [00:05<55:14,  4.52it/s, lr=0.0001, step_loss=0.00212]07/17/2023 19:22:30 - INFO - __main__ - Starting training step 19, global step 19\n",
      "07/17/2023 19:22:30 - INFO - __main__ - train loss is 0.008241001516580582\n",
      "Steps:   0%|     | 20/15000 [00:05<54:50,  4.55it/s, lr=0.0001, step_loss=0.033]07/17/2023 19:22:30 - INFO - __main__ - Starting training step 20, global step 20\n",
      "07/17/2023 19:22:30 - INFO - __main__ - train loss is 0.00436055613681674\n",
      "Steps:   0%|    | 21/15000 [00:05<54:47,  4.56it/s, lr=0.0001, step_loss=0.0174]07/17/2023 19:22:30 - INFO - __main__ - Starting training step 21, global step 21\n",
      "07/17/2023 19:22:30 - INFO - __main__ - train loss is 0.04614483192563057\n",
      "Steps:   0%|     | 22/15000 [00:06<54:32,  4.58it/s, lr=0.0001, step_loss=0.185]07/17/2023 19:22:30 - INFO - __main__ - Starting training step 22, global step 22\n",
      "07/17/2023 19:22:31 - INFO - __main__ - train loss is 0.002578441984951496\n",
      "Steps:   0%|    | 23/15000 [00:06<54:40,  4.57it/s, lr=0.0001, step_loss=0.0103]07/17/2023 19:22:31 - INFO - __main__ - Starting training step 23, global step 23\n",
      "07/17/2023 19:22:31 - INFO - __main__ - train loss is 0.024723365902900696\n",
      "Steps:   0%|    | 24/15000 [00:06<54:44,  4.56it/s, lr=0.0001, step_loss=0.0989]07/17/2023 19:22:31 - INFO - __main__ - Starting training step 24, global step 24\n",
      "07/17/2023 19:22:31 - INFO - __main__ - train loss is 0.0019778814166784286\n",
      "Steps:   0%|   | 25/15000 [00:06<54:46,  4.56it/s, lr=0.0001, step_loss=0.00791]07/17/2023 19:22:31 - INFO - __main__ - Starting training step 25, global step 25\n",
      "07/17/2023 19:22:31 - INFO - __main__ - train loss is 0.06838333606719971\n",
      "Steps:   0%|     | 26/15000 [00:07<54:50,  4.55it/s, lr=0.0001, step_loss=0.274]07/17/2023 19:22:31 - INFO - __main__ - Starting training step 26, global step 26\n",
      "07/17/2023 19:22:31 - INFO - __main__ - train loss is 0.060268811881542206\n",
      "Steps:   0%|     | 27/15000 [00:07<54:55,  4.54it/s, lr=0.0001, step_loss=0.241]07/17/2023 19:22:32 - INFO - __main__ - Starting training step 27, global step 27\n",
      "07/17/2023 19:22:32 - INFO - __main__ - train loss is 0.11838807165622711\n",
      "Steps:   0%|     | 28/15000 [00:07<54:51,  4.55it/s, lr=0.0001, step_loss=0.474]07/17/2023 19:22:32 - INFO - __main__ - Starting training step 28, global step 28\n",
      "07/17/2023 19:22:32 - INFO - __main__ - train loss is 0.0034486944787204266\n",
      "Steps:   0%|    | 29/15000 [00:07<54:51,  4.55it/s, lr=0.0001, step_loss=0.0138]07/17/2023 19:22:32 - INFO - __main__ - Starting training step 29, global step 29\n",
      "07/17/2023 19:22:32 - INFO - __main__ - train loss is 0.015431007370352745\n",
      "Steps:   0%|    | 30/15000 [00:07<54:44,  4.56it/s, lr=0.0001, step_loss=0.0617]07/17/2023 19:22:32 - INFO - __main__ - Starting training step 30, global step 30\n",
      "07/17/2023 19:22:32 - INFO - __main__ - train loss is 0.014218403026461601\n",
      "Steps:   0%|    | 31/15000 [00:08<54:22,  4.59it/s, lr=0.0001, step_loss=0.0569]07/17/2023 19:22:32 - INFO - __main__ - Starting training step 31, global step 31\n",
      "07/17/2023 19:22:33 - INFO - __main__ - train loss is 0.006709532346576452\n",
      "Steps:   0%|    | 32/15000 [00:08<54:44,  4.56it/s, lr=0.0001, step_loss=0.0268]07/17/2023 19:22:33 - INFO - __main__ - Starting training step 32, global step 32\n",
      "07/17/2023 19:22:33 - INFO - __main__ - train loss is 0.011136491782963276\n",
      "Steps:   0%|    | 33/15000 [00:08<54:48,  4.55it/s, lr=0.0001, step_loss=0.0445]07/17/2023 19:22:33 - INFO - __main__ - Starting training step 33, global step 33\n",
      "07/17/2023 19:22:33 - INFO - __main__ - train loss is 0.004567011259496212\n",
      "Steps:   0%|    | 34/15000 [00:08<54:39,  4.56it/s, lr=0.0001, step_loss=0.0183]07/17/2023 19:22:33 - INFO - __main__ - Starting training step 34, global step 34\n",
      "07/17/2023 19:22:33 - INFO - __main__ - train loss is 0.07410202920436859\n",
      "Steps:   0%|     | 35/15000 [00:08<54:33,  4.57it/s, lr=0.0001, step_loss=0.296]07/17/2023 19:22:33 - INFO - __main__ - Starting training step 35, global step 35\n",
      "07/17/2023 19:22:33 - INFO - __main__ - train loss is 0.10177023708820343\n",
      "Steps:   0%|     | 36/15000 [00:09<55:08,  4.52it/s, lr=0.0001, step_loss=0.407]07/17/2023 19:22:34 - INFO - __main__ - Starting training step 36, global step 36\n",
      "07/17/2023 19:22:34 - INFO - __main__ - train loss is 0.0873476192355156\n",
      "Steps:   0%|     | 37/15000 [00:09<55:08,  4.52it/s, lr=0.0001, step_loss=0.349]07/17/2023 19:22:34 - INFO - __main__ - Starting training step 37, global step 37\n",
      "07/17/2023 19:22:34 - INFO - __main__ - train loss is 0.05523793771862984\n",
      "Steps:   0%|     | 38/15000 [00:09<55:07,  4.52it/s, lr=0.0001, step_loss=0.221]07/17/2023 19:22:34 - INFO - __main__ - Starting training step 38, global step 38\n",
      "07/17/2023 19:22:34 - INFO - __main__ - train loss is 0.0020536952652037144\n",
      "Steps:   0%|   | 39/15000 [00:09<54:42,  4.56it/s, lr=0.0001, step_loss=0.00821]07/17/2023 19:22:34 - INFO - __main__ - Starting training step 39, global step 39\n",
      "07/17/2023 19:22:34 - INFO - __main__ - train loss is 0.0035235988907516003\n",
      "Steps:   0%|    | 40/15000 [00:10<54:54,  4.54it/s, lr=0.0001, step_loss=0.0141]07/17/2023 19:22:34 - INFO - __main__ - Starting training step 40, global step 40\n",
      "07/17/2023 19:22:35 - INFO - __main__ - train loss is 0.06853747367858887\n",
      "Steps:   0%|     | 41/15000 [00:10<54:47,  4.55it/s, lr=0.0001, step_loss=0.274]07/17/2023 19:22:35 - INFO - __main__ - Starting training step 41, global step 41\n",
      "07/17/2023 19:22:35 - INFO - __main__ - train loss is 0.0005353409214876592\n",
      "Steps:   0%|   | 42/15000 [00:10<54:49,  4.55it/s, lr=0.0001, step_loss=0.00214]07/17/2023 19:22:35 - INFO - __main__ - Starting training step 42, global step 42\n",
      "07/17/2023 19:22:35 - INFO - __main__ - train loss is 0.0005482578417286277\n",
      "Steps:   0%|   | 43/15000 [00:10<54:37,  4.56it/s, lr=0.0001, step_loss=0.00219]07/17/2023 19:22:35 - INFO - __main__ - Starting training step 43, global step 43\n",
      "07/17/2023 19:22:35 - INFO - __main__ - train loss is 0.0005305572412908077\n",
      "Steps:   0%|   | 44/15000 [00:10<54:53,  4.54it/s, lr=0.0001, step_loss=0.00212]07/17/2023 19:22:35 - INFO - __main__ - Starting training step 44, global step 44\n",
      "07/17/2023 19:22:35 - INFO - __main__ - train loss is 0.01943894289433956\n",
      "Steps:   0%|    | 45/15000 [00:11<54:47,  4.55it/s, lr=0.0001, step_loss=0.0778]07/17/2023 19:22:36 - INFO - __main__ - Starting training step 45, global step 45\n",
      "07/17/2023 19:22:36 - INFO - __main__ - train loss is 0.07614301145076752\n",
      "Steps:   0%|     | 46/15000 [00:11<54:38,  4.56it/s, lr=0.0001, step_loss=0.305]07/17/2023 19:22:36 - INFO - __main__ - Starting training step 46, global step 46\n",
      "07/17/2023 19:22:36 - INFO - __main__ - train loss is 0.12293115258216858\n",
      "Steps:   0%|     | 46/15000 [00:11<54:38,  4.56it/s, lr=0.0001, step_loss=0.492][2023-07-17 19:22:36,439] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "Steps:   0%|     | 47/15000 [00:11<54:27,  4.58it/s, lr=0.0001, step_loss=0.492]07/17/2023 19:22:36 - INFO - __main__ - Starting training step 47, global step 47\n",
      "07/17/2023 19:22:36 - INFO - __main__ - train loss is 0.012353646568953991\n",
      "Steps:   0%|    | 48/15000 [00:11<54:31,  4.57it/s, lr=0.0001, step_loss=0.0494]07/17/2023 19:22:36 - INFO - __main__ - Starting training step 48, global step 48\n",
      "07/17/2023 19:22:36 - INFO - __main__ - train loss is 0.002887976123020053\n",
      "Steps:   0%|    | 49/15000 [00:12<54:46,  4.55it/s, lr=0.0001, step_loss=0.0116]07/17/2023 19:22:36 - INFO - __main__ - Starting training step 49, global step 49\n",
      "07/17/2023 19:22:37 - INFO - __main__ - train loss is 0.0011723835486918688\n",
      "Steps:   0%|   | 50/15000 [00:12<54:43,  4.55it/s, lr=0.0001, step_loss=0.00469]07/17/2023 19:22:37 - INFO - __main__ - Starting training step 50, global step 50\n",
      "07/17/2023 19:22:37 - INFO - __main__ - train loss is 0.021326858550310135\n",
      "Steps:   0%|    | 51/15000 [00:12<54:50,  4.54it/s, lr=0.0001, step_loss=0.0853]07/17/2023 19:22:37 - INFO - __main__ - Starting training step 51, global step 51\n",
      "07/17/2023 19:22:37 - INFO - __main__ - train loss is 0.028330087661743164\n",
      "Steps:   0%|     | 52/15000 [00:12<55:02,  4.53it/s, lr=0.0001, step_loss=0.113]07/17/2023 19:22:37 - INFO - __main__ - Starting training step 52, global step 52\n",
      "07/17/2023 19:22:37 - INFO - __main__ - train loss is 0.001975536812096834\n",
      "Steps:   0%|    | 53/15000 [00:12<55:05,  4.52it/s, lr=0.0001, step_loss=0.0079]07/17/2023 19:22:37 - INFO - __main__ - Starting training step 53, global step 53\n",
      "07/17/2023 19:22:37 - INFO - __main__ - train loss is 0.06362202763557434\n",
      "Steps:   0%|     | 54/15000 [00:13<54:57,  4.53it/s, lr=0.0001, step_loss=0.254]07/17/2023 19:22:37 - INFO - __main__ - Starting training step 54, global step 54\n",
      "07/17/2023 19:22:38 - INFO - __main__ - train loss is 0.022695645689964294\n",
      "Steps:   0%|    | 55/15000 [00:13<54:42,  4.55it/s, lr=0.0001, step_loss=0.0908]07/17/2023 19:22:38 - INFO - __main__ - Starting training step 55, global step 55\n",
      "07/17/2023 19:22:38 - INFO - __main__ - train loss is 0.07761858403682709\n",
      "Steps:   0%|      | 56/15000 [00:13<54:48,  4.54it/s, lr=0.0001, step_loss=0.31]07/17/2023 19:22:38 - INFO - __main__ - Starting training step 56, global step 56\n",
      "07/17/2023 19:22:38 - INFO - __main__ - train loss is 0.09852202236652374\n",
      "Steps:   0%|     | 57/15000 [00:13<54:43,  4.55it/s, lr=0.0001, step_loss=0.394]07/17/2023 19:22:38 - INFO - __main__ - Starting training step 57, global step 57\n",
      "07/17/2023 19:22:38 - INFO - __main__ - train loss is 0.055006757378578186\n",
      "Steps:   0%|      | 58/15000 [00:14<54:49,  4.54it/s, lr=0.0001, step_loss=0.22]07/17/2023 19:22:38 - INFO - __main__ - Starting training step 58, global step 58\n",
      "07/17/2023 19:22:39 - INFO - __main__ - train loss is 0.06310956180095673\n",
      "Steps:   0%|     | 59/15000 [00:14<55:05,  4.52it/s, lr=0.0001, step_loss=0.252]07/17/2023 19:22:39 - INFO - __main__ - Starting training step 59, global step 59\n",
      "07/17/2023 19:22:39 - INFO - __main__ - train loss is 0.037720199674367905\n",
      "Steps:   0%|     | 60/15000 [00:14<55:10,  4.51it/s, lr=0.0001, step_loss=0.151]07/17/2023 19:22:39 - INFO - __main__ - Starting training step 60, global step 60\n",
      "07/17/2023 19:22:39 - INFO - __main__ - train loss is 0.0008895024657249451\n",
      "Steps:   0%|   | 61/15000 [00:14<54:49,  4.54it/s, lr=0.0001, step_loss=0.00356]07/17/2023 19:22:39 - INFO - __main__ - Starting training step 61, global step 61\n",
      "07/17/2023 19:22:39 - INFO - __main__ - train loss is 0.10724180191755295\n",
      "Steps:   0%|     | 62/15000 [00:14<54:42,  4.55it/s, lr=0.0001, step_loss=0.429]07/17/2023 19:22:39 - INFO - __main__ - Starting training step 62, global step 62\n",
      "07/17/2023 19:22:39 - INFO - __main__ - train loss is 0.0047597624361515045\n",
      "Steps:   0%|     | 63/15000 [00:15<54:51,  4.54it/s, lr=0.0001, step_loss=0.019]07/17/2023 19:22:39 - INFO - __main__ - Starting training step 63, global step 63\n",
      "07/17/2023 19:22:40 - INFO - __main__ - train loss is 0.0005530377384275198\n",
      "Steps:   0%|   | 64/15000 [00:15<54:54,  4.53it/s, lr=0.0001, step_loss=0.00221]07/17/2023 19:22:40 - INFO - __main__ - Starting training step 64, global step 64\n",
      "07/17/2023 19:22:40 - INFO - __main__ - train loss is 0.0643203929066658\n",
      "Steps:   0%|     | 65/15000 [00:15<55:02,  4.52it/s, lr=0.0001, step_loss=0.257]07/17/2023 19:22:40 - INFO - __main__ - Starting training step 65, global step 65\n",
      "07/17/2023 19:22:40 - INFO - __main__ - train loss is 0.024247517809271812\n",
      "Steps:   0%|     | 66/15000 [00:15<54:58,  4.53it/s, lr=0.0001, step_loss=0.097]07/17/2023 19:22:40 - INFO - __main__ - Starting training step 66, global step 66\n",
      "07/17/2023 19:22:40 - INFO - __main__ - train loss is 0.10019027441740036\n",
      "Steps:   0%|     | 67/15000 [00:16<55:04,  4.52it/s, lr=0.0001, step_loss=0.401]07/17/2023 19:22:40 - INFO - __main__ - Starting training step 67, global step 67\n",
      "07/17/2023 19:22:40 - INFO - __main__ - train loss is 0.005156322382390499\n",
      "Steps:   0%|    | 68/15000 [00:16<54:52,  4.54it/s, lr=0.0001, step_loss=0.0206]07/17/2023 19:22:41 - INFO - __main__ - Starting training step 68, global step 68\n",
      "07/17/2023 19:22:41 - INFO - __main__ - train loss is 0.051764801144599915\n",
      "Steps:   0%|     | 69/15000 [00:16<54:25,  4.57it/s, lr=0.0001, step_loss=0.207]07/17/2023 19:22:41 - INFO - __main__ - Starting training step 69, global step 69\n",
      "07/17/2023 19:22:41 - INFO - __main__ - train loss is 0.12030819058418274\n",
      "Steps:   0%|     | 70/15000 [00:16<54:18,  4.58it/s, lr=0.0001, step_loss=0.481]07/17/2023 19:22:41 - INFO - __main__ - Starting training step 70, global step 70\n",
      "07/17/2023 19:22:41 - INFO - __main__ - train loss is 0.0053154947236180305\n",
      "Steps:   0%|    | 71/15000 [00:16<54:38,  4.55it/s, lr=0.0001, step_loss=0.0213]07/17/2023 19:22:41 - INFO - __main__ - Starting training step 71, global step 71\n",
      "07/17/2023 19:22:41 - INFO - __main__ - train loss is 0.03449578210711479\n",
      "Steps:   0%|     | 72/15000 [00:17<54:58,  4.53it/s, lr=0.0001, step_loss=0.138]07/17/2023 19:22:41 - INFO - __main__ - Starting training step 72, global step 72\n",
      "07/17/2023 19:22:42 - INFO - __main__ - train loss is 0.08421127498149872\n",
      "Steps:   0%|     | 73/15000 [00:17<55:00,  4.52it/s, lr=0.0001, step_loss=0.337]07/17/2023 19:22:42 - INFO - __main__ - Starting training step 73, global step 73\n",
      "07/17/2023 19:22:42 - INFO - __main__ - train loss is 0.04559639096260071\n",
      "Steps:   0%|     | 74/15000 [00:17<55:05,  4.51it/s, lr=0.0001, step_loss=0.182]07/17/2023 19:22:42 - INFO - __main__ - Starting training step 74, global step 74\n",
      "07/17/2023 19:22:42 - INFO - __main__ - train loss is 0.0031022895127534866\n",
      "Steps:   0%|    | 75/15000 [00:17<55:01,  4.52it/s, lr=0.0001, step_loss=0.0124]07/17/2023 19:22:42 - INFO - __main__ - Starting training step 75, global step 75\n",
      "07/17/2023 19:22:42 - INFO - __main__ - train loss is 0.0004707750922534615\n",
      "Steps:   1%|   | 76/15000 [00:18<54:56,  4.53it/s, lr=0.0001, step_loss=0.00188]07/17/2023 19:22:42 - INFO - __main__ - Starting training step 76, global step 76\n",
      "07/17/2023 19:22:42 - INFO - __main__ - train loss is 0.11019453406333923\n",
      "Steps:   1%|     | 77/15000 [00:18<55:02,  4.52it/s, lr=0.0001, step_loss=0.441]07/17/2023 19:22:43 - INFO - __main__ - Starting training step 77, global step 77\n",
      "07/17/2023 19:22:43 - INFO - __main__ - train loss is 0.010169075801968575\n",
      "Steps:   1%|    | 78/15000 [00:18<54:54,  4.53it/s, lr=0.0001, step_loss=0.0407]07/17/2023 19:22:43 - INFO - __main__ - Starting training step 78, global step 78\n",
      "07/17/2023 19:22:43 - INFO - __main__ - train loss is 0.0014335170853883028\n",
      "Steps:   1%|   | 79/15000 [00:18<54:55,  4.53it/s, lr=0.0001, step_loss=0.00573]07/17/2023 19:22:43 - INFO - __main__ - Starting training step 79, global step 79\n",
      "07/17/2023 19:22:43 - INFO - __main__ - train loss is 0.006079145707190037\n",
      "Steps:   1%|    | 80/15000 [00:18<54:36,  4.55it/s, lr=0.0001, step_loss=0.0243]07/17/2023 19:22:43 - INFO - __main__ - Starting training step 80, global step 80\n",
      "07/17/2023 19:22:43 - INFO - __main__ - train loss is 0.053957194089889526\n",
      "Steps:   1%|     | 81/15000 [00:19<54:10,  4.59it/s, lr=0.0001, step_loss=0.216]07/17/2023 19:22:43 - INFO - __main__ - Starting training step 81, global step 81\n",
      "07/17/2023 19:22:44 - INFO - __main__ - train loss is 0.02774176374077797\n",
      "Steps:   1%|     | 82/15000 [00:19<54:24,  4.57it/s, lr=0.0001, step_loss=0.111]07/17/2023 19:22:44 - INFO - __main__ - Starting training step 82, global step 82\n",
      "07/17/2023 19:22:44 - INFO - __main__ - train loss is 0.006330566946417093\n",
      "Steps:   1%|    | 83/15000 [00:19<54:25,  4.57it/s, lr=0.0001, step_loss=0.0253]07/17/2023 19:22:44 - INFO - __main__ - Starting training step 83, global step 83\n",
      "07/17/2023 19:22:44 - INFO - __main__ - train loss is 0.046256937086582184\n",
      "Steps:   1%|     | 84/15000 [00:19<54:36,  4.55it/s, lr=0.0001, step_loss=0.185]07/17/2023 19:22:44 - INFO - __main__ - Starting training step 84, global step 84\n",
      "07/17/2023 19:22:44 - INFO - __main__ - train loss is 0.0006322104600258172\n",
      "Steps:   1%|   | 85/15000 [00:19<54:58,  4.52it/s, lr=0.0001, step_loss=0.00253]07/17/2023 19:22:44 - INFO - __main__ - Starting training step 85, global step 85\n",
      "07/17/2023 19:22:44 - INFO - __main__ - train loss is 0.2090340554714203\n",
      "Steps:   1%|     | 86/15000 [00:20<54:41,  4.55it/s, lr=0.0001, step_loss=0.836]07/17/2023 19:22:45 - INFO - __main__ - Starting training step 86, global step 86\n",
      "07/17/2023 19:22:45 - INFO - __main__ - train loss is 0.024798087775707245\n",
      "Steps:   1%|    | 87/15000 [00:20<54:35,  4.55it/s, lr=0.0001, step_loss=0.0992]07/17/2023 19:22:45 - INFO - __main__ - Starting training step 87, global step 87\n",
      "07/17/2023 19:22:45 - INFO - __main__ - train loss is 0.04383967071771622\n",
      "Steps:   1%|     | 88/15000 [00:20<54:28,  4.56it/s, lr=0.0001, step_loss=0.175]07/17/2023 19:22:45 - INFO - __main__ - Starting training step 88, global step 88\n",
      "07/17/2023 19:22:45 - INFO - __main__ - train loss is 0.06937234103679657\n",
      "Steps:   1%|     | 89/15000 [00:20<54:16,  4.58it/s, lr=0.0001, step_loss=0.277]07/17/2023 19:22:45 - INFO - __main__ - Starting training step 89, global step 89\n",
      "07/17/2023 19:22:45 - INFO - __main__ - train loss is 0.05049298703670502\n",
      "Steps:   1%|     | 90/15000 [00:21<54:28,  4.56it/s, lr=0.0001, step_loss=0.202]07/17/2023 19:22:45 - INFO - __main__ - Starting training step 90, global step 90\n",
      "07/17/2023 19:22:46 - INFO - __main__ - train loss is 0.002725380240008235\n",
      "Steps:   1%|    | 91/15000 [00:21<54:07,  4.59it/s, lr=0.0001, step_loss=0.0109]07/17/2023 19:22:46 - INFO - __main__ - Starting training step 91, global step 91\n",
      "07/17/2023 19:22:46 - INFO - __main__ - train loss is 0.22067883610725403\n",
      "Steps:   1%|     | 92/15000 [00:21<54:08,  4.59it/s, lr=0.0001, step_loss=0.883]07/17/2023 19:22:46 - INFO - __main__ - Starting training step 92, global step 92\n",
      "07/17/2023 19:22:46 - INFO - __main__ - train loss is 0.14167341589927673\n",
      "Steps:   1%|     | 93/15000 [00:21<54:17,  4.58it/s, lr=0.0001, step_loss=0.567]07/17/2023 19:22:46 - INFO - __main__ - Starting training step 93, global step 93\n",
      "07/17/2023 19:22:46 - INFO - __main__ - train loss is 0.06417953222990036\n",
      "Steps:   1%|     | 94/15000 [00:21<54:28,  4.56it/s, lr=0.0001, step_loss=0.257]07/17/2023 19:22:46 - INFO - __main__ - Starting training step 94, global step 94\n",
      "07/17/2023 19:22:46 - INFO - __main__ - train loss is 0.06838397681713104\n",
      "Steps:   1%|     | 95/15000 [00:22<54:31,  4.56it/s, lr=0.0001, step_loss=0.274]07/17/2023 19:22:47 - INFO - __main__ - Starting training step 95, global step 95\n",
      "07/17/2023 19:22:47 - INFO - __main__ - train loss is 0.05774331092834473\n",
      "Steps:   1%|     | 96/15000 [00:22<54:43,  4.54it/s, lr=0.0001, step_loss=0.231]07/17/2023 19:22:47 - INFO - __main__ - Starting training step 96, global step 96\n",
      "07/17/2023 19:22:47 - INFO - __main__ - train loss is 0.08841648697853088\n",
      "Steps:   1%|   | 97/15000 [00:22<1:05:45,  3.78it/s, lr=0.0001, step_loss=0.354]07/17/2023 19:22:47 - INFO - __main__ - Starting val step 0, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 0.0031973416917026043\n",
      "Steps:   1%|  | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.0032]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 1, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 0.20447617722675204\n",
      "Steps:   1%|   | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.201]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 2, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 0.3223559553734958\n",
      "Steps:   1%|   | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.118]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 3, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 0.8903746302239597\n",
      "Steps:   1%|   | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.568]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 4, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 1.2146148975007236\n",
      "Steps:   1%|   | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.324]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 5, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 1.2530358578078449\n",
      "Steps:   1%|  | 97/15000 [00:23<1:05:45,  3.78it/s, validation_step_loss=0.0384]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 6, global step 97\n",
      "07/17/2023 19:22:48 - INFO - __main__ - Val step loss is 1.5754755879752338\n",
      "Steps:   1%|   | 97/15000 [00:24<1:05:45,  3.78it/s, validation_step_loss=0.322]07/17/2023 19:22:48 - INFO - __main__ - Starting val step 7, global step 97\n",
      "07/17/2023 19:22:49 - INFO - __main__ - Val step loss is 1.6187098394148052\n",
      "Steps:   1%|  | 97/15000 [00:24<1:05:45,  3.78it/s, validation_step_loss=0.0432]07/17/2023 19:22:49 - INFO - __main__ - Starting val step 8, global step 97\n",
      "07/17/2023 19:22:49 - INFO - __main__ - Val step loss is 1.7174691851250827\n",
      "Steps:   1%|  | 97/15000 [00:24<1:05:45,  3.78it/s, validation_step_loss=0.0988]07/17/2023 19:22:49 - INFO - __main__ - Starting val step 9, global step 97\n",
      "07/17/2023 19:22:49 - INFO - __main__ - Val step loss is 2.074335544835776\n",
      "Steps:   1%|   | 97/15000 [00:24<1:05:45,  3.78it/s, validation_step_loss=0.357]07/17/2023 19:22:49 - INFO - __main__ - Starting val step 10, global step 97\n",
      "07/17/2023 19:22:49 - INFO - __main__ - Val step loss is 2.0781464364845306\n",
      "Steps:   1%| | 97/15000 [00:24<1:05:45,  3.78it/s, validation_step_loss=0.00381]07/17/2023 19:22:49 - INFO - __main__ - Starting val step 11, global step 97\n",
      "07/17/2023 19:22:49 - INFO - __main__ - Val step loss is 2.1657911625225097\n",
      "Steps:   1%|  | 97/15000 [00:25<1:05:45,  3.78it/s, validation_step_loss=0.0876]07/17/2023 19:22:49 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A spectrogram of bird song.\n",
      "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "07/17/2023 19:23:03 - INFO - __main__ - Starting epoch 1\n",
      "07/17/2023 19:23:03 - INFO - __main__ - Starting training step 0, global step 97\n",
      "07/17/2023 19:23:03 - INFO - __main__ - train loss is 0.0675840824842453\n",
      "Steps:   1%|   | 98/15000 [00:39<21:11:29,  5.12s/it, lr=0.0001, step_loss=0.27]07/17/2023 19:23:04 - INFO - __main__ - Starting training step 1, global step 98\n",
      "07/17/2023 19:23:04 - INFO - __main__ - train loss is 0.022446908056735992\n",
      "Steps:   1%| | 99/15000 [00:39<15:06:19,  3.65s/it, lr=0.0001, step_loss=0.0898]07/17/2023 19:23:04 - INFO - __main__ - Starting training step 2, global step 99\n",
      "07/17/2023 19:23:04 - INFO - __main__ - train loss is 0.05798554793000221\n",
      "Steps:   1%| | 100/15000 [00:39<10:51:45,  2.62s/it, lr=0.0001, step_loss=0.232]07/17/2023 19:23:04 - INFO - __main__ - Starting training step 3, global step 100\n",
      "07/17/2023 19:23:04 - INFO - __main__ - train loss is 0.005023346282541752\n",
      "Steps:   1%| | 101/15000 [00:39<7:54:23,  1.91s/it, lr=0.0001, step_loss=0.0201]07/17/2023 19:23:04 - INFO - __main__ - Starting training step 4, global step 101\n",
      "07/17/2023 19:23:04 - INFO - __main__ - train loss is 0.04141591489315033\n",
      "Steps:   1%|  | 102/15000 [00:40<5:48:47,  1.40s/it, lr=0.0001, step_loss=0.166]07/17/2023 19:23:04 - INFO - __main__ - Starting training step 5, global step 102\n",
      "07/17/2023 19:23:05 - INFO - __main__ - train loss is 0.0004642792046070099\n",
      "Steps:   1%| | 103/15000 [00:40<4:20:46,  1.05s/it, lr=0.0001, step_loss=0.0018607/17/2023 19:23:05 - INFO - __main__ - Starting training step 6, global step 103\n",
      "07/17/2023 19:23:05 - INFO - __main__ - train loss is 0.053756698966026306\n",
      "Steps:   1%|  | 104/15000 [00:40<3:19:15,  1.25it/s, lr=0.0001, step_loss=0.215]07/17/2023 19:23:05 - INFO - __main__ - Starting training step 7, global step 104\n",
      "07/17/2023 19:23:05 - INFO - __main__ - train loss is 0.03007223643362522\n",
      "Steps:   1%|   | 105/15000 [00:40<2:35:41,  1.59it/s, lr=0.0001, step_loss=0.12]07/17/2023 19:23:05 - INFO - __main__ - Starting training step 8, global step 105\n",
      "07/17/2023 19:23:05 - INFO - __main__ - train loss is 0.014584288001060486\n",
      "Steps:   1%| | 106/15000 [00:41<2:05:34,  1.98it/s, lr=0.0001, step_loss=0.0583]07/17/2023 19:23:05 - INFO - __main__ - Starting training step 9, global step 106\n",
      "07/17/2023 19:23:05 - INFO - __main__ - train loss is 0.0321580171585083\n",
      "Steps:   1%|  | 107/15000 [00:41<1:44:15,  2.38it/s, lr=0.0001, step_loss=0.129]07/17/2023 19:23:06 - INFO - __main__ - Starting training step 10, global step 107\n",
      "07/17/2023 19:23:06 - INFO - __main__ - train loss is 0.016210440546274185\n",
      "Steps:   1%| | 108/15000 [00:41<1:29:41,  2.77it/s, lr=0.0001, step_loss=0.0648]07/17/2023 19:23:06 - INFO - __main__ - Starting training step 11, global step 108\n",
      "07/17/2023 19:23:06 - INFO - __main__ - train loss is 0.003113962709903717\n",
      "Steps:   1%| | 109/15000 [00:41<1:19:22,  3.13it/s, lr=0.0001, step_loss=0.0125]07/17/2023 19:23:06 - INFO - __main__ - Starting training step 12, global step 109\n",
      "07/17/2023 19:23:06 - INFO - __main__ - train loss is 0.05925511568784714\n",
      "Steps:   1%|  | 110/15000 [00:41<1:12:20,  3.43it/s, lr=0.0001, step_loss=0.237]07/17/2023 19:23:06 - INFO - __main__ - Starting training step 13, global step 110\n",
      "07/17/2023 19:23:06 - INFO - __main__ - train loss is 0.06354755163192749\n",
      "Steps:   1%|  | 111/15000 [00:42<1:07:10,  3.69it/s, lr=0.0001, step_loss=0.254]07/17/2023 19:23:06 - INFO - __main__ - Starting training step 14, global step 111\n",
      "07/17/2023 19:23:07 - INFO - __main__ - train loss is 0.0295429527759552\n",
      "Steps:   1%|  | 112/15000 [00:42<1:03:39,  3.90it/s, lr=0.0001, step_loss=0.118]07/17/2023 19:23:07 - INFO - __main__ - Starting training step 15, global step 112\n",
      "07/17/2023 19:23:07 - INFO - __main__ - train loss is 0.08484247326850891\n",
      "Steps:   1%|  | 113/15000 [00:42<1:01:07,  4.06it/s, lr=0.0001, step_loss=0.339]07/17/2023 19:23:07 - INFO - __main__ - Starting training step 16, global step 113\n",
      "07/17/2023 19:23:07 - INFO - __main__ - train loss is 0.05949024483561516\n",
      "Steps:   1%|    | 114/15000 [00:42<59:13,  4.19it/s, lr=0.0001, step_loss=0.238]07/17/2023 19:23:07 - INFO - __main__ - Starting training step 17, global step 114\n",
      "07/17/2023 19:23:07 - INFO - __main__ - train loss is 0.0007247385219670832\n",
      "Steps:   1%|   | 115/15000 [00:43<57:57,  4.28it/s, lr=0.0001, step_loss=0.0029]07/17/2023 19:23:07 - INFO - __main__ - Starting training step 18, global step 115\n",
      "07/17/2023 19:23:07 - INFO - __main__ - train loss is 0.08668599277734756\n",
      "Steps:   1%|    | 116/15000 [00:43<56:58,  4.35it/s, lr=0.0001, step_loss=0.347]07/17/2023 19:23:08 - INFO - __main__ - Starting training step 19, global step 116\n",
      "07/17/2023 19:23:08 - INFO - __main__ - train loss is 0.09377801418304443\n",
      "Steps:   1%|    | 117/15000 [00:43<57:05,  4.35it/s, lr=0.0001, step_loss=0.375]07/17/2023 19:23:08 - INFO - __main__ - Starting training step 20, global step 117\n",
      "07/17/2023 19:23:08 - INFO - __main__ - train loss is 0.0025490117259323597\n",
      "Steps:   1%|   | 118/15000 [00:43<56:29,  4.39it/s, lr=0.0001, step_loss=0.0102]07/17/2023 19:23:08 - INFO - __main__ - Starting training step 21, global step 118\n",
      "07/17/2023 19:23:08 - INFO - __main__ - train loss is 0.015634693205356598\n",
      "Steps:   1%|   | 119/15000 [00:43<56:05,  4.42it/s, lr=0.0001, step_loss=0.0625]07/17/2023 19:23:08 - INFO - __main__ - Starting training step 22, global step 119\n",
      "07/17/2023 19:23:08 - INFO - __main__ - train loss is 0.07962559163570404\n",
      "Steps:   1%|    | 120/15000 [00:44<55:36,  4.46it/s, lr=0.0001, step_loss=0.319]07/17/2023 19:23:08 - INFO - __main__ - Starting training step 23, global step 120\n",
      "07/17/2023 19:23:09 - INFO - __main__ - train loss is 0.02884172461926937\n",
      "Steps:   1%|    | 121/15000 [00:44<55:34,  4.46it/s, lr=0.0001, step_loss=0.115]07/17/2023 19:23:09 - INFO - __main__ - Starting training step 24, global step 121\n",
      "07/17/2023 19:23:09 - INFO - __main__ - train loss is 0.06923423707485199\n",
      "Steps:   1%|    | 122/15000 [00:44<55:40,  4.45it/s, lr=0.0001, step_loss=0.277]07/17/2023 19:23:09 - INFO - __main__ - Starting training step 25, global step 122\n",
      "07/17/2023 19:23:09 - INFO - __main__ - train loss is 0.01137510221451521\n",
      "Steps:   1%|   | 123/15000 [00:44<55:32,  4.46it/s, lr=0.0001, step_loss=0.0455]07/17/2023 19:23:09 - INFO - __main__ - Starting training step 26, global step 123\n",
      "07/17/2023 19:23:09 - INFO - __main__ - train loss is 0.09184303879737854\n",
      "Steps:   1%|    | 124/15000 [00:45<55:25,  4.47it/s, lr=0.0001, step_loss=0.367]07/17/2023 19:23:09 - INFO - __main__ - Starting training step 27, global step 124\n",
      "07/17/2023 19:23:09 - INFO - __main__ - train loss is 0.0004138915683142841\n",
      "Steps:   1%|  | 125/15000 [00:45<55:16,  4.48it/s, lr=0.0001, step_loss=0.00166]07/17/2023 19:23:10 - INFO - __main__ - Starting training step 28, global step 125\n",
      "07/17/2023 19:23:10 - INFO - __main__ - train loss is 0.003126556519418955\n",
      "Steps:   1%|   | 126/15000 [00:45<55:23,  4.47it/s, lr=0.0001, step_loss=0.0125]07/17/2023 19:23:10 - INFO - __main__ - Starting training step 29, global step 126\n",
      "07/17/2023 19:23:10 - INFO - __main__ - train loss is 0.11363118886947632\n",
      "Steps:   1%|    | 127/15000 [00:45<55:27,  4.47it/s, lr=0.0001, step_loss=0.455]07/17/2023 19:23:10 - INFO - __main__ - Starting training step 30, global step 127\n",
      "07/17/2023 19:23:10 - INFO - __main__ - train loss is 0.011064402759075165\n",
      "Steps:   1%|   | 128/15000 [00:45<55:32,  4.46it/s, lr=0.0001, step_loss=0.0443]07/17/2023 19:23:10 - INFO - __main__ - Starting training step 31, global step 128\n",
      "07/17/2023 19:23:10 - INFO - __main__ - train loss is 0.017867304384708405\n",
      "Steps:   1%|   | 129/15000 [00:46<55:34,  4.46it/s, lr=0.0001, step_loss=0.0715]07/17/2023 19:23:10 - INFO - __main__ - Starting training step 32, global step 129\n",
      "07/17/2023 19:23:11 - INFO - __main__ - train loss is 0.0287761352956295\n",
      "Steps:   1%|    | 130/15000 [00:46<55:37,  4.46it/s, lr=0.0001, step_loss=0.115]07/17/2023 19:23:11 - INFO - __main__ - Starting training step 33, global step 130\n",
      "07/17/2023 19:23:11 - INFO - __main__ - train loss is 0.012317754328250885\n",
      "Steps:   1%|   | 131/15000 [00:46<55:39,  4.45it/s, lr=0.0001, step_loss=0.0493]07/17/2023 19:23:11 - INFO - __main__ - Starting training step 34, global step 131\n",
      "07/17/2023 19:23:11 - INFO - __main__ - train loss is 0.014021363109350204\n",
      "Steps:   1%|   | 132/15000 [00:46<55:28,  4.47it/s, lr=0.0001, step_loss=0.0561]07/17/2023 19:23:11 - INFO - __main__ - Starting training step 35, global step 132\n",
      "07/17/2023 19:23:11 - INFO - __main__ - train loss is 0.0055511086247861385\n",
      "Steps:   1%|   | 133/15000 [00:47<55:04,  4.50it/s, lr=0.0001, step_loss=0.0222]07/17/2023 19:23:11 - INFO - __main__ - Starting training step 36, global step 133\n",
      "07/17/2023 19:23:12 - INFO - __main__ - train loss is 0.0011634672991931438\n",
      "Steps:   1%|  | 134/15000 [00:47<55:15,  4.48it/s, lr=0.0001, step_loss=0.00465]07/17/2023 19:23:12 - INFO - __main__ - Starting training step 37, global step 134\n",
      "07/17/2023 19:23:12 - INFO - __main__ - train loss is 0.0077863894402980804\n",
      "Steps:   1%|   | 135/15000 [00:47<55:05,  4.50it/s, lr=0.0001, step_loss=0.0311]07/17/2023 19:23:12 - INFO - __main__ - Starting training step 38, global step 135\n",
      "07/17/2023 19:23:12 - INFO - __main__ - train loss is 0.007914314046502113\n",
      "Steps:   1%|   | 136/15000 [00:47<55:14,  4.48it/s, lr=0.0001, step_loss=0.0317]07/17/2023 19:23:12 - INFO - __main__ - Starting training step 39, global step 136\n",
      "07/17/2023 19:23:12 - INFO - __main__ - train loss is 0.0005380271468311548\n",
      "Steps:   1%|  | 137/15000 [00:47<55:12,  4.49it/s, lr=0.0001, step_loss=0.00215]07/17/2023 19:23:12 - INFO - __main__ - Starting training step 40, global step 137\n",
      "07/17/2023 19:23:12 - INFO - __main__ - train loss is 0.1512729525566101\n",
      "Steps:   1%|    | 138/15000 [00:48<55:06,  4.49it/s, lr=0.0001, step_loss=0.605]07/17/2023 19:23:12 - INFO - __main__ - Starting training step 41, global step 138\n",
      "07/17/2023 19:23:13 - INFO - __main__ - train loss is 0.024600520730018616\n",
      "Steps:   1%|   | 139/15000 [00:48<55:03,  4.50it/s, lr=0.0001, step_loss=0.0984]07/17/2023 19:23:13 - INFO - __main__ - Starting training step 42, global step 139\n",
      "07/17/2023 19:23:13 - INFO - __main__ - train loss is 0.0111018605530262\n",
      "Steps:   1%|   | 140/15000 [00:48<55:22,  4.47it/s, lr=0.0001, step_loss=0.0444]07/17/2023 19:23:13 - INFO - __main__ - Starting training step 43, global step 140\n",
      "07/17/2023 19:23:13 - INFO - __main__ - train loss is 0.07502155005931854\n",
      "Steps:   1%|      | 141/15000 [00:48<55:22,  4.47it/s, lr=0.0001, step_loss=0.3]07/17/2023 19:23:13 - INFO - __main__ - Starting training step 44, global step 141\n",
      "07/17/2023 19:23:13 - INFO - __main__ - train loss is 0.02641931176185608\n",
      "Steps:   1%|    | 142/15000 [00:49<55:14,  4.48it/s, lr=0.0001, step_loss=0.106]07/17/2023 19:23:13 - INFO - __main__ - Starting training step 45, global step 142\n",
      "07/17/2023 19:23:14 - INFO - __main__ - train loss is 0.1146930530667305\n",
      "Steps:   1%|    | 143/15000 [00:49<55:07,  4.49it/s, lr=0.0001, step_loss=0.459]07/17/2023 19:23:14 - INFO - __main__ - Starting training step 46, global step 143\n",
      "07/17/2023 19:23:14 - INFO - __main__ - train loss is 0.08323700726032257\n",
      "Steps:   1%|    | 144/15000 [00:49<55:23,  4.47it/s, lr=0.0001, step_loss=0.333]07/17/2023 19:23:14 - INFO - __main__ - Starting training step 47, global step 144\n",
      "07/17/2023 19:23:14 - INFO - __main__ - train loss is 0.0013973278691992164\n",
      "Steps:   1%|  | 145/15000 [00:49<55:38,  4.45it/s, lr=0.0001, step_loss=0.00559]07/17/2023 19:23:14 - INFO - __main__ - Starting training step 48, global step 145\n",
      "07/17/2023 19:23:14 - INFO - __main__ - train loss is 0.007679796777665615\n",
      "Steps:   1%|   | 146/15000 [00:49<55:31,  4.46it/s, lr=0.0001, step_loss=0.0307]07/17/2023 19:23:14 - INFO - __main__ - Starting training step 49, global step 146\n",
      "07/17/2023 19:23:14 - INFO - __main__ - train loss is 0.12296400964260101\n",
      "Steps:   1%|    | 147/15000 [00:50<55:26,  4.47it/s, lr=0.0001, step_loss=0.492]07/17/2023 19:23:15 - INFO - __main__ - Starting training step 50, global step 147\n",
      "07/17/2023 19:23:15 - INFO - __main__ - train loss is 0.0015355200739577413\n",
      "Steps:   1%|  | 148/15000 [00:50<55:20,  4.47it/s, lr=0.0001, step_loss=0.00614]07/17/2023 19:23:15 - INFO - __main__ - Starting training step 51, global step 148\n",
      "07/17/2023 19:23:15 - INFO - __main__ - train loss is 0.023162169381976128\n",
      "Steps:   1%|   | 149/15000 [00:50<55:26,  4.47it/s, lr=0.0001, step_loss=0.0926]07/17/2023 19:23:15 - INFO - __main__ - Starting training step 52, global step 149\n",
      "07/17/2023 19:23:15 - INFO - __main__ - train loss is 0.0011788113042712212\n",
      "Steps:   1%|  | 150/15000 [00:50<54:56,  4.51it/s, lr=0.0001, step_loss=0.00472]07/17/2023 19:23:15 - INFO - __main__ - Starting training step 53, global step 150\n",
      "07/17/2023 19:23:15 - INFO - __main__ - train loss is 0.10967852175235748\n",
      "Steps:   1%|    | 151/15000 [00:51<54:58,  4.50it/s, lr=0.0001, step_loss=0.439]07/17/2023 19:23:15 - INFO - __main__ - Starting training step 54, global step 151\n",
      "07/17/2023 19:23:16 - INFO - __main__ - train loss is 0.00277226441539824\n",
      "Steps:   1%|   | 152/15000 [00:51<54:57,  4.50it/s, lr=0.0001, step_loss=0.0111]07/17/2023 19:23:16 - INFO - __main__ - Starting training step 55, global step 152\n",
      "07/17/2023 19:23:16 - INFO - __main__ - train loss is 0.05341118574142456\n",
      "Steps:   1%|    | 153/15000 [00:51<54:38,  4.53it/s, lr=0.0001, step_loss=0.214]07/17/2023 19:23:16 - INFO - __main__ - Starting training step 56, global step 153\n",
      "07/17/2023 19:23:16 - INFO - __main__ - train loss is 0.06695335358381271\n",
      "Steps:   1%|    | 154/15000 [00:51<54:24,  4.55it/s, lr=0.0001, step_loss=0.268]07/17/2023 19:23:16 - INFO - __main__ - Starting training step 57, global step 154\n",
      "07/17/2023 19:23:16 - INFO - __main__ - train loss is 0.05365784093737602\n",
      "Steps:   1%|    | 155/15000 [00:51<54:18,  4.56it/s, lr=0.0001, step_loss=0.215]07/17/2023 19:23:16 - INFO - __main__ - Starting training step 58, global step 155\n",
      "07/17/2023 19:23:16 - INFO - __main__ - train loss is 0.025902213528752327\n",
      "Steps:   1%|    | 156/15000 [00:52<54:31,  4.54it/s, lr=0.0001, step_loss=0.104]07/17/2023 19:23:16 - INFO - __main__ - Starting training step 59, global step 156\n",
      "07/17/2023 19:23:17 - INFO - __main__ - train loss is 0.0016419251915067434\n",
      "Steps:   1%|  | 157/15000 [00:52<54:46,  4.52it/s, lr=0.0001, step_loss=0.00657]07/17/2023 19:23:17 - INFO - __main__ - Starting training step 60, global step 157\n",
      "07/17/2023 19:23:17 - INFO - __main__ - train loss is 0.046411339193582535\n",
      "Steps:   1%|    | 158/15000 [00:52<54:37,  4.53it/s, lr=0.0001, step_loss=0.186]07/17/2023 19:23:17 - INFO - __main__ - Starting training step 61, global step 158\n",
      "07/17/2023 19:23:17 - INFO - __main__ - train loss is 0.048830606043338776\n",
      "Steps:   1%|    | 159/15000 [00:52<54:35,  4.53it/s, lr=0.0001, step_loss=0.195]07/17/2023 19:23:17 - INFO - __main__ - Starting training step 62, global step 159\n",
      "07/17/2023 19:23:17 - INFO - __main__ - train loss is 0.03744196519255638\n",
      "Steps:   1%|     | 160/15000 [00:53<54:33,  4.53it/s, lr=0.0001, step_loss=0.15]07/17/2023 19:23:17 - INFO - __main__ - Starting training step 63, global step 160\n",
      "07/17/2023 19:23:18 - INFO - __main__ - train loss is 0.0059206802397966385\n",
      "Steps:   1%|   | 161/15000 [00:53<54:34,  4.53it/s, lr=0.0001, step_loss=0.0237]07/17/2023 19:23:18 - INFO - __main__ - Starting training step 64, global step 161\n",
      "07/17/2023 19:23:18 - INFO - __main__ - train loss is 0.10193236172199249\n",
      "Steps:   1%|    | 162/15000 [00:53<54:31,  4.54it/s, lr=0.0001, step_loss=0.408]07/17/2023 19:23:18 - INFO - __main__ - Starting training step 65, global step 162\n",
      "07/17/2023 19:23:18 - INFO - __main__ - train loss is 0.014325639232993126\n",
      "Steps:   1%|   | 163/15000 [00:53<55:06,  4.49it/s, lr=0.0001, step_loss=0.0573]07/17/2023 19:23:18 - INFO - __main__ - Starting training step 66, global step 163\n",
      "07/17/2023 19:23:18 - INFO - __main__ - train loss is 0.0042554596439003944\n",
      "Steps:   1%|    | 164/15000 [00:53<55:23,  4.46it/s, lr=0.0001, step_loss=0.017]07/17/2023 19:23:18 - INFO - __main__ - Starting training step 67, global step 164\n",
      "07/17/2023 19:23:18 - INFO - __main__ - train loss is 0.0056691644713282585\n",
      "Steps:   1%|   | 165/15000 [00:54<55:53,  4.42it/s, lr=0.0001, step_loss=0.0227]07/17/2023 19:23:19 - INFO - __main__ - Starting training step 68, global step 165\n",
      "07/17/2023 19:23:19 - INFO - __main__ - train loss is 0.07539930939674377\n",
      "Steps:   1%|    | 166/15000 [00:54<55:28,  4.46it/s, lr=0.0001, step_loss=0.302]07/17/2023 19:23:19 - INFO - __main__ - Starting training step 69, global step 166\n",
      "07/17/2023 19:23:19 - INFO - __main__ - train loss is 0.04974662512540817\n",
      "Steps:   1%|    | 167/15000 [00:54<55:23,  4.46it/s, lr=0.0001, step_loss=0.199]07/17/2023 19:23:19 - INFO - __main__ - Starting training step 70, global step 167\n",
      "07/17/2023 19:23:19 - INFO - __main__ - train loss is 0.002131116110831499\n",
      "Steps:   1%|  | 168/15000 [00:54<55:58,  4.42it/s, lr=0.0001, step_loss=0.00852]07/17/2023 19:23:19 - INFO - __main__ - Starting training step 71, global step 168\n",
      "07/17/2023 19:23:19 - INFO - __main__ - train loss is 0.05475790053606033\n",
      "Steps:   1%|    | 169/15000 [00:55<56:26,  4.38it/s, lr=0.0001, step_loss=0.219]07/17/2023 19:23:19 - INFO - __main__ - Starting training step 72, global step 169\n",
      "07/17/2023 19:23:20 - INFO - __main__ - train loss is 0.09398005157709122\n",
      "Steps:   1%|    | 170/15000 [00:55<56:04,  4.41it/s, lr=0.0001, step_loss=0.376]07/17/2023 19:23:20 - INFO - __main__ - Starting training step 73, global step 170\n",
      "07/17/2023 19:23:20 - INFO - __main__ - train loss is 0.06769806146621704\n",
      "Steps:   1%|    | 171/15000 [00:55<55:41,  4.44it/s, lr=0.0001, step_loss=0.271]07/17/2023 19:23:20 - INFO - __main__ - Starting training step 74, global step 171\n",
      "07/17/2023 19:23:20 - INFO - __main__ - train loss is 0.014066778123378754\n",
      "Steps:   1%|   | 172/15000 [00:55<55:23,  4.46it/s, lr=0.0001, step_loss=0.0563]07/17/2023 19:23:20 - INFO - __main__ - Starting training step 75, global step 172\n",
      "07/17/2023 19:23:20 - INFO - __main__ - train loss is 0.029336463660001755\n",
      "Steps:   1%|    | 173/15000 [00:55<55:18,  4.47it/s, lr=0.0001, step_loss=0.117]07/17/2023 19:23:20 - INFO - __main__ - Starting training step 76, global step 173\n",
      "07/17/2023 19:23:20 - INFO - __main__ - train loss is 0.10532793402671814\n",
      "Steps:   1%|    | 174/15000 [00:56<55:23,  4.46it/s, lr=0.0001, step_loss=0.421]07/17/2023 19:23:21 - INFO - __main__ - Starting training step 77, global step 174\n",
      "07/17/2023 19:23:21 - INFO - __main__ - train loss is 0.0037123742513358593\n",
      "Steps:   1%|   | 175/15000 [00:56<55:17,  4.47it/s, lr=0.0001, step_loss=0.0148]07/17/2023 19:23:21 - INFO - __main__ - Starting training step 78, global step 175\n",
      "07/17/2023 19:23:21 - INFO - __main__ - train loss is 0.008161784149706364\n",
      "Steps:   1%|   | 176/15000 [00:56<56:27,  4.38it/s, lr=0.0001, step_loss=0.0326]07/17/2023 19:23:21 - INFO - __main__ - Starting training step 79, global step 176\n",
      "07/17/2023 19:23:21 - INFO - __main__ - train loss is 0.001558057963848114\n",
      "Steps:   1%|  | 177/15000 [00:56<56:48,  4.35it/s, lr=0.0001, step_loss=0.00623]07/17/2023 19:23:21 - INFO - __main__ - Starting training step 80, global step 177\n",
      "07/17/2023 19:23:21 - INFO - __main__ - train loss is 0.1423269510269165\n",
      "Steps:   1%|    | 178/15000 [00:57<57:02,  4.33it/s, lr=0.0001, step_loss=0.569]07/17/2023 19:23:21 - INFO - __main__ - Starting training step 81, global step 178\n",
      "07/17/2023 19:23:22 - INFO - __main__ - train loss is 0.07380472123622894\n",
      "Steps:   1%|    | 179/15000 [00:57<56:35,  4.36it/s, lr=0.0001, step_loss=0.295]07/17/2023 19:23:22 - INFO - __main__ - Starting training step 82, global step 179\n",
      "07/17/2023 19:23:22 - INFO - __main__ - train loss is 0.04087001085281372\n",
      "Steps:   1%|    | 180/15000 [00:57<56:47,  4.35it/s, lr=0.0001, step_loss=0.163]07/17/2023 19:23:22 - INFO - __main__ - Starting training step 83, global step 180\n",
      "07/17/2023 19:23:22 - INFO - __main__ - train loss is 0.043752752244472504\n",
      "Steps:   1%|    | 181/15000 [00:57<56:57,  4.34it/s, lr=0.0001, step_loss=0.175]07/17/2023 19:23:22 - INFO - __main__ - Starting training step 84, global step 181\n",
      "07/17/2023 19:23:22 - INFO - __main__ - train loss is 0.04900309443473816\n",
      "Steps:   1%|    | 182/15000 [00:58<56:19,  4.38it/s, lr=0.0001, step_loss=0.196]07/17/2023 19:23:22 - INFO - __main__ - Starting training step 85, global step 182\n",
      "07/17/2023 19:23:22 - INFO - __main__ - train loss is 0.025485042482614517\n",
      "Steps:   1%|    | 183/15000 [00:58<55:40,  4.44it/s, lr=0.0001, step_loss=0.102]07/17/2023 19:23:23 - INFO - __main__ - Starting training step 86, global step 183\n",
      "07/17/2023 19:23:23 - INFO - __main__ - train loss is 0.000676092691719532\n",
      "Steps:   1%|   | 184/15000 [00:58<55:26,  4.45it/s, lr=0.0001, step_loss=0.0027]07/17/2023 19:23:23 - INFO - __main__ - Starting training step 87, global step 184\n",
      "07/17/2023 19:23:23 - INFO - __main__ - train loss is 0.0017614340176805854\n",
      "Steps:   1%|  | 185/15000 [00:58<55:16,  4.47it/s, lr=0.0001, step_loss=0.00705]07/17/2023 19:23:23 - INFO - __main__ - Starting training step 88, global step 185\n",
      "07/17/2023 19:23:23 - INFO - __main__ - train loss is 0.05488326773047447\n",
      "Steps:   1%|     | 186/15000 [00:58<54:59,  4.49it/s, lr=0.0001, step_loss=0.22]07/17/2023 19:23:23 - INFO - __main__ - Starting training step 89, global step 186\n",
      "07/17/2023 19:23:23 - INFO - __main__ - train loss is 0.0639985203742981\n",
      "Steps:   1%|    | 187/15000 [00:59<55:09,  4.48it/s, lr=0.0001, step_loss=0.256]07/17/2023 19:23:23 - INFO - __main__ - Starting training step 90, global step 187\n",
      "07/17/2023 19:23:24 - INFO - __main__ - train loss is 0.04201092943549156\n",
      "Steps:   1%|    | 188/15000 [00:59<55:07,  4.48it/s, lr=0.0001, step_loss=0.168]07/17/2023 19:23:24 - INFO - __main__ - Starting training step 91, global step 188\n",
      "07/17/2023 19:23:24 - INFO - __main__ - train loss is 0.00446982029825449\n",
      "Steps:   1%|   | 189/15000 [00:59<54:40,  4.51it/s, lr=0.0001, step_loss=0.0179]07/17/2023 19:23:24 - INFO - __main__ - Starting training step 92, global step 189\n",
      "07/17/2023 19:23:24 - INFO - __main__ - train loss is 0.0009499297011643648\n",
      "Steps:   1%|   | 190/15000 [00:59<54:53,  4.50it/s, lr=0.0001, step_loss=0.0038]07/17/2023 19:23:24 - INFO - __main__ - Starting training step 93, global step 190\n",
      "07/17/2023 19:23:24 - INFO - __main__ - train loss is 0.019178850576281548\n",
      "Steps:   1%|   | 191/15000 [01:00<55:04,  4.48it/s, lr=0.0001, step_loss=0.0767]07/17/2023 19:23:24 - INFO - __main__ - Starting training step 94, global step 191\n",
      "07/17/2023 19:23:24 - INFO - __main__ - train loss is 0.0005207703216001391\n",
      "Steps:   1%|  | 192/15000 [01:00<55:00,  4.49it/s, lr=0.0001, step_loss=0.00208]07/17/2023 19:23:25 - INFO - __main__ - Starting training step 95, global step 192\n",
      "07/17/2023 19:23:25 - INFO - __main__ - train loss is 0.05362848937511444\n",
      "Steps:   1%|    | 193/15000 [01:00<55:39,  4.43it/s, lr=0.0001, step_loss=0.215]07/17/2023 19:23:25 - INFO - __main__ - Starting training step 96, global step 193\n",
      "07/17/2023 19:23:25 - INFO - __main__ - train loss is 0.0013799740700051188\n",
      "Steps:   1%| | 194/15000 [01:00<1:09:25,  3.55it/s, lr=0.0001, step_loss=0.0055207/17/2023 19:23:26 - INFO - __main__ - Starting val step 0, global step 194\n",
      "07/17/2023 19:23:26 - INFO - __main__ - Val step loss is 0.009592527523636818\n",
      "Steps:   1%| | 194/15000 [01:01<1:09:25,  3.55it/s, validation_step_loss=0.0095907/17/2023 19:23:26 - INFO - __main__ - Starting val step 1, global step 194\n",
      "07/17/2023 19:23:26 - INFO - __main__ - Val step loss is 0.2129220012575388\n",
      "Steps:   1%|  | 194/15000 [01:01<1:09:25,  3.55it/s, validation_step_loss=0.203]07/17/2023 19:23:26 - INFO - __main__ - Starting val step 2, global step 194\n",
      "07/17/2023 19:23:26 - INFO - __main__ - Val step loss is 0.216188570484519\n",
      "Steps:   1%| | 194/15000 [01:01<1:09:25,  3.55it/s, validation_step_loss=0.0032707/17/2023 19:23:26 - INFO - __main__ - Starting val step 3, global step 194\n",
      "07/17/2023 19:23:26 - INFO - __main__ - Val step loss is 0.22696133702993393\n",
      "Steps:   1%| | 194/15000 [01:01<1:09:25,  3.55it/s, validation_step_loss=0.0108]07/17/2023 19:23:26 - INFO - __main__ - Starting val step 4, global step 194\n",
      "07/17/2023 19:23:26 - INFO - __main__ - Val step loss is 0.25413574650883675\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0272]07/17/2023 19:23:26 - INFO - __main__ - Starting val step 5, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.27465272694826126\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0205]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 6, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.34053885191679\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0659]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 7, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.4206809848546982\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0801]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 8, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.7390664666891098\n",
      "Steps:   1%|  | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.318]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 9, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.7494512749835849\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0104]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 10, global step 194\n",
      "07/17/2023 19:23:27 - INFO - __main__ - Val step loss is 0.8442312581464648\n",
      "Steps:   1%| | 194/15000 [01:02<1:09:25,  3.55it/s, validation_step_loss=0.0948]07/17/2023 19:23:27 - INFO - __main__ - Starting val step 11, global step 194\n",
      "07/17/2023 19:23:28 - INFO - __main__ - Val step loss is 0.9947829646989703\n",
      "Steps:   1%|  | 194/15000 [01:03<1:09:25,  3.55it/s, validation_step_loss=0.151]07/17/2023 19:23:28 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A spectrogram of bird song.\n",
      "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "07/17/2023 19:23:41 - INFO - __main__ - Starting epoch 2\n",
      "07/17/2023 19:23:41 - INFO - __main__ - Starting training step 0, global step 194\n",
      "07/17/2023 19:23:42 - INFO - __main__ - train loss is 0.024769678711891174\n",
      "Steps:   1%| | 195/15000 [01:17<21:07:17,  5.14s/it, lr=0.0001, step_loss=0.099107/17/2023 19:23:42 - INFO - __main__ - Starting training step 1, global step 195\n",
      "07/17/2023 19:23:42 - INFO - __main__ - train loss is 0.0005878655938431621\n",
      "Steps:   1%| | 196/15000 [01:17<15:03:29,  3.66s/it, lr=0.0001, step_loss=0.002307/17/2023 19:23:42 - INFO - __main__ - Starting training step 2, global step 196\n",
      "07/17/2023 19:23:42 - INFO - __main__ - train loss is 0.0008334583835676312\n",
      "Steps:   1%| | 197/15000 [01:17<10:49:53,  2.63s/it, lr=0.0001, step_loss=0.003307/17/2023 19:23:42 - INFO - __main__ - Starting training step 3, global step 197\n",
      "07/17/2023 19:23:42 - INFO - __main__ - train loss is 0.015951979905366898\n",
      "Steps:   1%| | 198/15000 [01:18<7:52:08,  1.91s/it, lr=0.0001, step_loss=0.0638]07/17/2023 19:23:42 - INFO - __main__ - Starting training step 4, global step 198\n",
      "07/17/2023 19:23:43 - INFO - __main__ - train loss is 0.00657038576900959\n",
      "Steps:   1%| | 199/15000 [01:18<5:47:01,  1.41s/it, lr=0.0001, step_loss=0.0263]07/17/2023 19:23:43 - INFO - __main__ - Starting training step 5, global step 199\n",
      "07/17/2023 19:23:43 - INFO - __main__ - train loss is 0.035993169993162155\n",
      "Steps:   1%|  | 200/15000 [01:18<4:19:12,  1.05s/it, lr=0.0001, step_loss=0.144]07/17/2023 19:23:43 - INFO - __main__ - Starting training step 6, global step 200\n",
      "07/17/2023 19:23:43 - INFO - __main__ - train loss is 0.024852247908711433\n",
      "Steps:   1%| | 201/15000 [01:18<3:17:51,  1.25it/s, lr=0.0001, step_loss=0.0994]07/17/2023 19:23:43 - INFO - __main__ - Starting training step 7, global step 201\n",
      "07/17/2023 19:23:43 - INFO - __main__ - train loss is 0.017530785873532295\n",
      "Steps:   1%| | 202/15000 [01:18<2:34:30,  1.60it/s, lr=0.0001, step_loss=0.0701]07/17/2023 19:23:43 - INFO - __main__ - Starting training step 8, global step 202\n",
      "07/17/2023 19:23:43 - INFO - __main__ - train loss is 0.008212916553020477\n",
      "Steps:   1%| | 203/15000 [01:19<2:04:32,  1.98it/s, lr=0.0001, step_loss=0.0329]07/17/2023 19:23:43 - INFO - __main__ - Starting training step 9, global step 203\n",
      "07/17/2023 19:23:44 - INFO - __main__ - train loss is 0.0020126253366470337\n",
      "Steps:   1%| | 204/15000 [01:19<1:44:15,  2.37it/s, lr=0.0001, step_loss=0.0080507/17/2023 19:23:44 - INFO - __main__ - Starting training step 10, global step 204\n",
      "07/17/2023 19:23:44 - INFO - __main__ - train loss is 0.0031935786828398705\n",
      "Steps:   1%| | 205/15000 [01:19<1:29:46,  2.75it/s, lr=0.0001, step_loss=0.0128]07/17/2023 19:23:44 - INFO - __main__ - Starting training step 11, global step 205\n",
      "07/17/2023 19:23:44 - INFO - __main__ - train loss is 0.15825682878494263\n",
      "Steps:   1%|  | 206/15000 [01:19<1:19:18,  3.11it/s, lr=0.0001, step_loss=0.633]07/17/2023 19:23:44 - INFO - __main__ - Starting training step 12, global step 206\n",
      "07/17/2023 19:23:44 - INFO - __main__ - train loss is 0.030522283166646957\n",
      "Steps:   1%|  | 207/15000 [01:20<1:11:42,  3.44it/s, lr=0.0001, step_loss=0.122]07/17/2023 19:23:44 - INFO - __main__ - Starting training step 13, global step 207\n",
      "07/17/2023 19:23:45 - INFO - __main__ - train loss is 0.001996312988922\n",
      "Steps:   1%| | 208/15000 [01:20<1:06:47,  3.69it/s, lr=0.0001, step_loss=0.0079907/17/2023 19:23:45 - INFO - __main__ - Starting training step 14, global step 208\n",
      "07/17/2023 19:23:45 - INFO - __main__ - train loss is 0.03231406956911087\n",
      "Steps:   1%|  | 209/15000 [01:20<1:03:25,  3.89it/s, lr=0.0001, step_loss=0.129]07/17/2023 19:23:45 - INFO - __main__ - Starting training step 15, global step 209\n",
      "07/17/2023 19:23:45 - INFO - __main__ - train loss is 0.035380516201257706\n",
      "Steps:   1%|  | 210/15000 [01:20<1:00:36,  4.07it/s, lr=0.0001, step_loss=0.142]07/17/2023 19:23:45 - INFO - __main__ - Starting training step 16, global step 210\n",
      "07/17/2023 19:23:45 - INFO - __main__ - train loss is 0.010327816009521484\n",
      "Steps:   1%|   | 211/15000 [01:20<59:30,  4.14it/s, lr=0.0001, step_loss=0.0413]07/17/2023 19:23:45 - INFO - __main__ - Starting training step 17, global step 211\n",
      "07/17/2023 19:23:45 - INFO - __main__ - train loss is 0.022765222936868668\n",
      "Steps:   1%|   | 212/15000 [01:21<58:11,  4.24it/s, lr=0.0001, step_loss=0.0911]07/17/2023 19:23:46 - INFO - __main__ - Starting training step 18, global step 212\n",
      "07/17/2023 19:23:46 - INFO - __main__ - train loss is 0.0004935794859193265\n",
      "Steps:   1%|  | 213/15000 [01:21<57:16,  4.30it/s, lr=0.0001, step_loss=0.00197]07/17/2023 19:23:46 - INFO - __main__ - Starting training step 19, global step 213\n",
      "07/17/2023 19:23:46 - INFO - __main__ - train loss is 0.14284995198249817\n",
      "Steps:   1%|    | 214/15000 [01:21<56:32,  4.36it/s, lr=0.0001, step_loss=0.571]07/17/2023 19:23:46 - INFO - __main__ - Starting training step 20, global step 214\n",
      "07/17/2023 19:23:46 - INFO - __main__ - train loss is 0.0005234353011474013\n",
      "Steps:   1%|  | 215/15000 [01:21<56:07,  4.39it/s, lr=0.0001, step_loss=0.00209]07/17/2023 19:23:46 - INFO - __main__ - Starting training step 21, global step 215\n",
      "07/17/2023 19:23:46 - INFO - __main__ - train loss is 0.008220924064517021\n",
      "Steps:   1%|   | 216/15000 [01:22<55:25,  4.45it/s, lr=0.0001, step_loss=0.0329]07/17/2023 19:23:46 - INFO - __main__ - Starting training step 22, global step 216\n",
      "07/17/2023 19:23:47 - INFO - __main__ - train loss is 0.11205260455608368\n",
      "Steps:   1%|    | 217/15000 [01:22<55:13,  4.46it/s, lr=0.0001, step_loss=0.448]07/17/2023 19:23:47 - INFO - __main__ - Starting training step 23, global step 217\n",
      "07/17/2023 19:23:47 - INFO - __main__ - train loss is 0.006477107293903828\n",
      "Steps:   1%|   | 218/15000 [01:22<55:19,  4.45it/s, lr=0.0001, step_loss=0.0259]07/17/2023 19:23:47 - INFO - __main__ - Starting training step 24, global step 218\n",
      "07/17/2023 19:23:47 - INFO - __main__ - train loss is 0.009097935631871223\n",
      "Steps:   1%|   | 219/15000 [01:22<55:14,  4.46it/s, lr=0.0001, step_loss=0.0364]07/17/2023 19:23:47 - INFO - __main__ - Starting training step 25, global step 219\n",
      "07/17/2023 19:23:47 - INFO - __main__ - train loss is 0.0271515641361475\n",
      "Steps:   1%|    | 220/15000 [01:22<54:58,  4.48it/s, lr=0.0001, step_loss=0.109]07/17/2023 19:23:47 - INFO - __main__ - Starting training step 26, global step 220\n",
      "07/17/2023 19:23:47 - INFO - __main__ - train loss is 0.0344826802611351\n",
      "Steps:   1%|    | 221/15000 [01:23<55:00,  4.48it/s, lr=0.0001, step_loss=0.138]07/17/2023 19:23:48 - INFO - __main__ - Starting training step 27, global step 221\n",
      "07/17/2023 19:23:48 - INFO - __main__ - train loss is 0.0015474152751266956\n",
      "Steps:   1%|  | 222/15000 [01:23<55:03,  4.47it/s, lr=0.0001, step_loss=0.00619]07/17/2023 19:23:48 - INFO - __main__ - Starting training step 28, global step 222\n",
      "07/17/2023 19:23:48 - INFO - __main__ - train loss is 0.01931091956794262\n",
      "Steps:   1%|   | 223/15000 [01:23<55:04,  4.47it/s, lr=0.0001, step_loss=0.0772]07/17/2023 19:23:48 - INFO - __main__ - Starting training step 29, global step 223\n",
      "07/17/2023 19:23:48 - INFO - __main__ - train loss is 0.09175039827823639\n",
      "Steps:   1%|    | 224/15000 [01:23<54:48,  4.49it/s, lr=0.0001, step_loss=0.367]07/17/2023 19:23:48 - INFO - __main__ - Starting training step 30, global step 224\n",
      "07/17/2023 19:23:48 - INFO - __main__ - train loss is 0.029751572757959366\n",
      "Steps:   2%|    | 225/15000 [01:24<54:39,  4.50it/s, lr=0.0001, step_loss=0.119]07/17/2023 19:23:48 - INFO - __main__ - Starting training step 31, global step 225\n",
      "07/17/2023 19:23:49 - INFO - __main__ - train loss is 0.09087375551462173\n",
      "Steps:   2%|    | 226/15000 [01:24<54:43,  4.50it/s, lr=0.0001, step_loss=0.363]07/17/2023 19:23:49 - INFO - __main__ - Starting training step 32, global step 226\n",
      "07/17/2023 19:23:49 - INFO - __main__ - train loss is 0.011590899899601936\n",
      "Steps:   2%|   | 227/15000 [01:24<54:50,  4.49it/s, lr=0.0001, step_loss=0.0464]07/17/2023 19:23:49 - INFO - __main__ - Starting training step 33, global step 227\n",
      "07/17/2023 19:23:49 - INFO - __main__ - train loss is 0.12609590590000153\n",
      "Steps:   2%|    | 228/15000 [01:24<54:46,  4.49it/s, lr=0.0001, step_loss=0.504]07/17/2023 19:23:49 - INFO - __main__ - Starting training step 34, global step 228\n",
      "07/17/2023 19:23:49 - INFO - __main__ - train loss is 0.0015059574507176876\n",
      "Steps:   2%|  | 229/15000 [01:24<54:55,  4.48it/s, lr=0.0001, step_loss=0.00602]07/17/2023 19:23:49 - INFO - __main__ - Starting training step 35, global step 229\n",
      "07/17/2023 19:23:49 - INFO - __main__ - train loss is 0.07038921117782593\n",
      "Steps:   2%|    | 230/15000 [01:25<54:51,  4.49it/s, lr=0.0001, step_loss=0.282]07/17/2023 19:23:50 - INFO - __main__ - Starting training step 36, global step 230\n",
      "07/17/2023 19:23:50 - INFO - __main__ - train loss is 0.007178925909101963\n",
      "Steps:   2%|   | 231/15000 [01:25<54:49,  4.49it/s, lr=0.0001, step_loss=0.0287]07/17/2023 19:23:50 - INFO - __main__ - Starting training step 37, global step 231\n",
      "07/17/2023 19:23:50 - INFO - __main__ - train loss is 0.10037505626678467\n",
      "Steps:   2%|    | 232/15000 [01:25<55:42,  4.42it/s, lr=0.0001, step_loss=0.402]07/17/2023 19:23:50 - INFO - __main__ - Starting training step 38, global step 232\n",
      "07/17/2023 19:23:50 - INFO - __main__ - train loss is 0.08956501632928848\n",
      "Steps:   2%|    | 233/15000 [01:25<55:07,  4.47it/s, lr=0.0001, step_loss=0.358]07/17/2023 19:23:50 - INFO - __main__ - Starting training step 39, global step 233\n",
      "07/17/2023 19:23:50 - INFO - __main__ - train loss is 0.0005363927921280265\n",
      "Steps:   2%|  | 234/15000 [01:26<54:48,  4.49it/s, lr=0.0001, step_loss=0.00215]07/17/2023 19:23:50 - INFO - __main__ - Starting training step 40, global step 234\n",
      "07/17/2023 19:23:51 - INFO - __main__ - train loss is 0.0040388149209320545\n",
      "Steps:   2%|   | 235/15000 [01:26<54:53,  4.48it/s, lr=0.0001, step_loss=0.0162]07/17/2023 19:23:51 - INFO - __main__ - Starting training step 41, global step 235\n",
      "07/17/2023 19:23:51 - INFO - __main__ - train loss is 0.005364703480154276\n",
      "Steps:   2%|   | 236/15000 [01:26<54:55,  4.48it/s, lr=0.0001, step_loss=0.0215]07/17/2023 19:23:51 - INFO - __main__ - Starting training step 42, global step 236\n",
      "07/17/2023 19:23:51 - INFO - __main__ - train loss is 0.0003796679084189236\n",
      "Steps:   2%|  | 237/15000 [01:26<55:05,  4.47it/s, lr=0.0001, step_loss=0.00152]07/17/2023 19:23:51 - INFO - __main__ - Starting training step 43, global step 237\n",
      "07/17/2023 19:23:51 - INFO - __main__ - train loss is 0.10557429492473602\n",
      "Steps:   2%|    | 238/15000 [01:26<55:22,  4.44it/s, lr=0.0001, step_loss=0.422]07/17/2023 19:23:51 - INFO - __main__ - Starting training step 44, global step 238\n",
      "07/17/2023 19:23:51 - INFO - __main__ - train loss is 0.019824229180812836\n",
      "Steps:   2%|   | 239/15000 [01:27<55:07,  4.46it/s, lr=0.0001, step_loss=0.0793]07/17/2023 19:23:52 - INFO - __main__ - Starting training step 45, global step 239\n",
      "07/17/2023 19:23:52 - INFO - __main__ - train loss is 0.014402677305042744\n",
      "Steps:   2%|   | 240/15000 [01:27<54:57,  4.48it/s, lr=0.0001, step_loss=0.0576]07/17/2023 19:23:52 - INFO - __main__ - Starting training step 46, global step 240\n",
      "07/17/2023 19:23:52 - INFO - __main__ - train loss is 0.006658392958343029\n",
      "Steps:   2%|   | 241/15000 [01:27<54:59,  4.47it/s, lr=0.0001, step_loss=0.0266]07/17/2023 19:23:52 - INFO - __main__ - Starting training step 47, global step 241\n",
      "07/17/2023 19:23:52 - INFO - __main__ - train loss is 0.0011478954693302512\n",
      "Steps:   2%|  | 242/15000 [01:27<55:00,  4.47it/s, lr=0.0001, step_loss=0.00459]07/17/2023 19:23:52 - INFO - __main__ - Starting training step 48, global step 242\n",
      "07/17/2023 19:23:52 - INFO - __main__ - train loss is 0.05288741737604141\n",
      "Steps:   2%|    | 243/15000 [01:28<55:01,  4.47it/s, lr=0.0001, step_loss=0.212]07/17/2023 19:23:52 - INFO - __main__ - Starting training step 49, global step 243\n",
      "07/17/2023 19:23:53 - INFO - __main__ - train loss is 0.10442548990249634\n",
      "Steps:   2%|    | 244/15000 [01:28<55:01,  4.47it/s, lr=0.0001, step_loss=0.418]07/17/2023 19:23:53 - INFO - __main__ - Starting training step 50, global step 244\n",
      "07/17/2023 19:23:53 - INFO - __main__ - train loss is 0.0030855746008455753\n",
      "Steps:   2%|   | 245/15000 [01:28<54:45,  4.49it/s, lr=0.0001, step_loss=0.0123]07/17/2023 19:23:53 - INFO - __main__ - Starting training step 51, global step 245\n",
      "07/17/2023 19:23:53 - INFO - __main__ - train loss is 0.012350982055068016\n",
      "Steps:   2%|   | 246/15000 [01:28<54:43,  4.49it/s, lr=0.0001, step_loss=0.0494]07/17/2023 19:23:53 - INFO - __main__ - Starting training step 52, global step 246\n",
      "07/17/2023 19:23:53 - INFO - __main__ - train loss is 0.1576707810163498\n",
      "Steps:   2%|    | 247/15000 [01:28<54:39,  4.50it/s, lr=0.0001, step_loss=0.631]07/17/2023 19:23:53 - INFO - __main__ - Starting training step 53, global step 247\n",
      "^C\n",
      "\u001b[2;36m[19:23:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Received \u001b[1;36m2\u001b[0m death signal, shutting down workers    \u001b]8;id=834034;file:///home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py\u001b\\\u001b[2mapi.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=845060;file:///home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py#729\u001b\\\u001b[2m729\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Sending process \u001b[1;36m40403\u001b[0m closing signal SIGINT       \u001b]8;id=218304;file:///home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\u001b\\\u001b[2mapi.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=210630;file:///home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py#698\u001b\\\u001b[2m698\u001b[0m\u001b]8;;\u001b\\\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbea5e37d90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/PIL/ImageFile.py\", line 518, in _save\n",
      "    fh = fp.fileno()\n",
      "AttributeError: '_idat' object has no attribute 'fileno'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/diss/msc_diss/sdspeech/model/sd_ex/lora/train_text_to_image_lora.py\", line 1094, in <module>\n",
      "    main()\n",
      "  File \"/home/ryan/diss/msc_diss/sdspeech/model/sd_ex/lora/train_text_to_image_lora.py\", line 814, in main\n",
      "    wandb.log({\"train/training_images\": wandb.Image(batch[\"pixel_values\"])})\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/wandb/sdk/data_types/image.py\", line 162, in __init__\n",
      "    self._initialize_from_data(data_or_path, mode)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/wandb/sdk/data_types/image.py\", line 307, in _initialize_from_data\n",
      "    self._image.save(tmp_path, transparency=None)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/PIL/Image.py\", line 2431, in save\n",
      "    save_handler(self, fp, filename)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/PIL/PngImagePlugin.py\", line 1420, in _save\n",
      "    ImageFile._save(im, _idat(fp, chunk), [(\"zip\", (0, 0) + im.size, 0, rawmode)])\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/PIL/ImageFile.py\", line 522, in _save\n",
      "    _encode_tile(im, fp, tile, bufsize, None, exc)\n",
      "  File \"/home/ryan/miniconda3/envs/msc_diss/lib/python3.10/site-packages/PIL/ImageFile.py\", line 541, in _encode_tile\n",
      "    l, s, d = encoder.encode(bufsize)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run training script\n",
    "!accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --train_data_dir=\"/home/ryan/diss/msc_diss/sdspeech/data/AudioSet/spec/Bird vocalization-bird call-bird song/train\" \\\n",
    "  --val_data_dir=\"/home/ryan/diss/msc_diss/sdspeech/data/AudioSet/spec/Bird vocalization-bird call-bird song/val\" \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --val_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-04 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --output_dir=$\"./out/17-07\" \\\n",
    "  --report_to=wandb \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --validation_prompt=\"A spectrogram of bird song\" \\\n",
    "  --seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "!accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --train_data_dir=\"/home/ryan/diss/msc_diss/sdspeech/data/AudioSet/spec/Bird vocalization-bird call-bird song/train\" \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-05 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --output_dir=$\"./out/13-07/2\" \\\n",
    "  --report_to=wandb \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --validation_prompt=\"A spectrogram of bird song\" \\\n",
    "  --seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "\n",
    "# Load model\n",
    "model_base = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Set model to load fine-tuned weights\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.load_attn_procs(\"./out/26-06\")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "inf_out = \"inference/out/11-07/no_aug\"\n",
    "\n",
    "seeds = [0, 1, 42, 49, 55, 1337, 26000, 50000, 50101]\n",
    "\n",
    "prompt = \"a spectrogram of bird song\"\n",
    "\n",
    "for seed in seeds:\n",
    "    gen = torch.manual_seed(seed)\n",
    "    \n",
    "    # use half the weights from the LoRA finetuned model and half the weights from the base model\n",
    "    image = pipe(\n",
    "        prompt, num_inference_steps=25, guidance_scale=7.5, cross_attention_kwargs={\"scale\": 0}, generator=gen\n",
    "    ).images[0]\n",
    "    image.save(inf_out + prompt + \"_\" + str(seed) + \"_base\" + \".png\")\n",
    "    # use the weights from the fully finetuned LoRA model\n",
    "\n",
    "    image = pipe(prompt, num_inference_steps=25, guidance_scale=7.5).images[0]\n",
    "    image.save(inf_out + prompt + \"_\" + str(seed) + \"_lora\" + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "def display_images_in_grid(folder_path):\n",
    "    # Get a list of all image files in the folder\n",
    "    image_files = [file for file in os.listdir(folder_path) if file.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
    "    image_files.sort()  # Sort the image files in alphabetical order\n",
    "\n",
    "    # Set up the grid layout\n",
    "    num_images = len(image_files)\n",
    "    num_cols = 2  # Number of columns in the grid\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Number of rows based on the number of images\n",
    "\n",
    "    # Create a figure and axis objects\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "        # Adjust the spacing properties\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # Iterate over the image files and display them in the grid\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        # Compute the row and column index of the current image\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Load the image using Matplotlib's imread\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = imread(image_path)\n",
    "\n",
    "        # Display the image\n",
    "        axs[row, col].imshow(image)\n",
    "        axs[row, col].axis(\"off\")\n",
    "\n",
    "        # Set the filename as the title\n",
    "        \"\"\" filename = os.path.splitext(image_file)[0]\n",
    "        axs[row, col].set_title(filename, fontsize=8) \"\"\"\n",
    "\n",
    "    # Add column titles\n",
    "    axs[0, 0].set_title(\"Base\")\n",
    "    axs[0, 1].set_title(\"LoRA\")\n",
    "\n",
    "    # Adjust the spacing and layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the grid of images\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display inference\n",
    "\n",
    "# Specify the folder path where the images are located\n",
    "folder_path = \"./inference/05-06-spec-test/\"\n",
    "\n",
    "# Call the function to display images in a grid\n",
    "display_images_in_grid(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a72e5e1b89d6aca93f43995c0113c69f11f03ee989f354bc28ea86012bfefd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
